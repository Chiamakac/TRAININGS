{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practical Awesome-align-demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chiamakac/TRAININGS/blob/main/Alignment/Practical_Awesome_align_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjc7LvIQbuMn"
      },
      "source": [
        "# AWESOME: Aligning Word Embedding Spaces of Multilingual Encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ipxcuO9vDgZ"
      },
      "source": [
        "[``awesome-align``](https://github.com/neulab/awesome-align) is a tool that can extract word alignments from multilingual BERT (mBERT) and allows you to fine-tune mBERT on parallel corpora for better alignment quality (see [our paper](https://arxiv.org/abs/2101.08231) for more details).\n",
        "\n",
        "This is a simple demo of how `awesome-align` extracts word alignments from mBERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJpRK-1_wQsJ"
      },
      "source": [
        "First, install and import the following packages. (Note that the original `awesome-align` tool does not require the `transformers` package.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODwJ_gQ8bnqR",
        "outputId": "0fe3df69-c606-4081-d6e5-0003d89c36cb"
      },
      "source": [
        "!pip install transformers==3.1.0\n",
        "import torch\n",
        "import transformers\n",
        "import itertools"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==3.1.0 in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.1.96)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.8.1rc2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (3.4.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (0.0.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRvfawCbw2i7"
      },
      "source": [
        "Load the multilingual BERT model and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aPvLqT7eiry"
      },
      "source": [
        "model = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare input sentences\n",
        "\n",
        "\n",
        "1.   Read the English text file and store the sentences in a list `ensents = [en_sent1, en_sent2, ..., en_sentn]`\n",
        "2.   Do the same for the Igbo text file `igsents = [ig_sent1, ig_sent2, ..., ig_sentn]`\n",
        "\n",
        "3.   Store each sentence pair tuple in a list file `en_ig_sents = [(en_sent1, ig_sent1), (en_sent2, ig_sent2), ..., (en_sentn, ig_sentn)]\n",
        "\n"
      ],
      "metadata": {
        "id": "rjWB92jk0p-K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RPxlavmxNmj"
      },
      "source": [
        "Input *tokenized* source and target sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "for src, tgt in en_ig_sents:\n",
        "  # perform the alignment\n",
        "```"
      ],
      "metadata": {
        "id": "QOqzLNSg1FoS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfDM0w2kfHyJ"
      },
      "source": [
        "src = 'My name is Jessica, I am French and I am thirteen years old, I go to school in Nice, but I live in Cagnes-Sur-Mer..'\n",
        "tgt = 'Aha m bụ Jessica, abụ m French na adị m afọ iri na atọ, aga m akwụkwọ na Nice, mana m bi na Cagnes-Sur-Mer.'.split()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MUMHItzzf2Q",
        "outputId": "b34f01bd-7b68-4e9a-b370-1a659e60a13f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Aha',\n",
              " 'm',\n",
              " 'bụ',\n",
              " 'Jessica,',\n",
              " 'abụ',\n",
              " 'm',\n",
              " 'French',\n",
              " 'na',\n",
              " 'adị',\n",
              " 'm',\n",
              " 'afọ',\n",
              " 'iri',\n",
              " 'na',\n",
              " 'atọ,',\n",
              " 'aga',\n",
              " 'm',\n",
              " 'akwụkwọ',\n",
              " 'na',\n",
              " 'Nice,',\n",
              " 'mana',\n",
              " 'm',\n",
              " 'bi',\n",
              " 'na',\n",
              " 'Cagnes-Sur-Mer.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# src= 'onye ahụ nke nwụrụ bụ nke a mụrụ na ụbọchị nke iri na anọ na ọnwa Febụwarị na afọ 1923 na ezinaụlọ nke Pa/Odoziakụ Anakonwa laworo mmụọ nke ime obodo Oranto.'\n",
        "# tgt= 'The deceased was born on Feb. 14, 1923 into family of late Pa/Mrs Anakonwa of Oranto Village, married to late Pius Amatu of Obinagu Akpu Village'"
      ],
      "metadata": {
        "id": "CHJHtoUdmCgU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpUa-ZqUxZ8Z"
      },
      "source": [
        "Run the model and print the resulting alignments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smW6s5JJflCN",
        "outputId": "69bf9240-40a2-4b75-df66-6d66bc95282b"
      },
      "source": [
        "# pre-processing\n",
        "sent_src, sent_tgt = src.strip().split(), tgt.strip().split()\n",
        "token_src, token_tgt = [tokenizer.tokenize(word) for word in sent_src], [tokenizer.tokenize(word) for word in sent_tgt]\n",
        "wid_src, wid_tgt = [tokenizer.convert_tokens_to_ids(x) for x in token_src], [tokenizer.convert_tokens_to_ids(x) for x in token_tgt]\n",
        "ids_src, ids_tgt = tokenizer.prepare_for_model(list(itertools.chain(*wid_src)), return_tensors='pt', model_max_length=tokenizer.model_max_length, truncation=True)['input_ids'], tokenizer.prepare_for_model(list(itertools.chain(*wid_tgt)), return_tensors='pt', truncation=True, model_max_length=tokenizer.model_max_length)['input_ids']\n",
        "sub2word_map_src = []\n",
        "for i, word_list in enumerate(token_src):\n",
        "  sub2word_map_src += [i for x in word_list]\n",
        "sub2word_map_tgt = []\n",
        "for i, word_list in enumerate(token_tgt):\n",
        "  sub2word_map_tgt += [i for x in word_list]\n",
        "\n",
        "# alignment\n",
        "align_layer = 8\n",
        "threshold = 1e-3\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  out_src = model(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "  out_tgt = model(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]\n",
        "\n",
        "  dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))\n",
        "\n",
        "  softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)\n",
        "  softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)\n",
        "\n",
        "  softmax_inter = (softmax_srctgt > threshold)*(softmax_tgtsrc > threshold)\n",
        "\n",
        "align_subwords = torch.nonzero(softmax_inter, as_tuple=False)\n",
        "align_words = set()\n",
        "for i, j in align_subwords:\n",
        "  align_words.add( (sub2word_map_src[i], sub2word_map_tgt[j]) )\n",
        "\n",
        "# printing\n",
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'\n",
        "\n",
        "for i, j in sorted(align_words):\n",
        "  print(f'{color.BOLD}{color.BLUE}{sent_src[i]}{color.END}==={color.BOLD}{color.RED}{sent_tgt[j]}{color.END}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[94mMy\u001b[0m===\u001b[1m\u001b[91mAha\u001b[0m\n",
            "\u001b[1m\u001b[94mJessica,\u001b[0m===\u001b[1m\u001b[91mJessica,\u001b[0m\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mAha\u001b[0m\n",
            "\u001b[1m\u001b[94mam\u001b[0m===\u001b[1m\u001b[91mm\u001b[0m\n",
            "\u001b[1m\u001b[94mFrench\u001b[0m===\u001b[1m\u001b[91mFrench\u001b[0m\n",
            "\u001b[1m\u001b[94mthirteen\u001b[0m===\u001b[1m\u001b[91mbụ\u001b[0m\n",
            "\u001b[1m\u001b[94mold,\u001b[0m===\u001b[1m\u001b[91matọ,\u001b[0m\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mm\u001b[0m\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mm\u001b[0m\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mm\u001b[0m\n",
            "\u001b[1m\u001b[94mgo\u001b[0m===\u001b[1m\u001b[91miri\u001b[0m\n",
            "\u001b[1m\u001b[94mschool\u001b[0m===\u001b[1m\u001b[91miri\u001b[0m\n",
            "\u001b[1m\u001b[94min\u001b[0m===\u001b[1m\u001b[91mna\u001b[0m\n",
            "\u001b[1m\u001b[94mNice,\u001b[0m===\u001b[1m\u001b[91mNice,\u001b[0m\n",
            "\u001b[1m\u001b[94mI\u001b[0m===\u001b[1m\u001b[91mm\u001b[0m\n",
            "\u001b[1m\u001b[94mlive\u001b[0m===\u001b[1m\u001b[91mbi\u001b[0m\n",
            "\u001b[1m\u001b[94min\u001b[0m===\u001b[1m\u001b[91mna\u001b[0m\n",
            "\u001b[1m\u001b[94mCagnes-Sur-Mer..\u001b[0m===\u001b[1m\u001b[91mCagnes-Sur-Mer.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "alignment_dict = {\n",
        "  my:[aha, m, mu],\n",
        "  i: [aha, m, m, m, m] => aha=1, m=4 => i==m\n",
        "  microsoft: [microsoft, ...]\n",
        "\n",
        "} \n",
        "```"
      ],
      "metadata": {
        "id": "-w9BJHNt4TLB"
      }
    }
  ]
}